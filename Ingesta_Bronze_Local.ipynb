{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6683c8fe-b90f-4af8-9a91-89b78346ab4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definici√≥n de Widgets para par√°metros de entrada\n",
    "# El formato es: dbutils.widgets.text(\"nombre_variable\", \"valor_por_defecto\", \"Etiqueta Visual\")\n",
    "\n",
    "dbutils.widgets.text(\"aws_access_key\", \"\", \"1. AWS Access Key\")\n",
    "dbutils.widgets.text(\"aws_secret_key\", \"\", \"2. AWS Secret Key\")\n",
    "dbutils.widgets.text(\"s3_bucket_name\", \"nombre-de-tu-bucket\", \"3. Nombre del Bucket\")\n",
    "\n",
    "# Widget de lista desplegable para el entorno (esto da un toque Senior)\n",
    "dbutils.widgets.dropdown(\"environment\", \"dev\", [\"dev\", \"qa\", \"prod\"], \"4. Entorno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10fffc74-fdf4-4df7-8310-7848b591b433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Recuperar los valores ingresados por el usuario\n",
    "ACCESS_KEY = dbutils.widgets.get(\"aws_access_key\")\n",
    "SECRET_KEY = dbutils.widgets.get(\"aws_secret_key\")\n",
    "BUCKET_NAME = dbutils.widgets.get(\"s3_bucket_name\")\n",
    "env = dbutils.widgets.get(\"environment\")\n",
    "\n",
    "# Validaci√≥n de seguridad\n",
    "if not access_key or not secret_key:\n",
    "    dbutils.notebook.exit(\"Error: Debes ingresar las credenciales de AWS en los widgets superiores.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Configuraci√≥n cargada exitosamente para el entorno: {env}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3b78aa-3d94-486b-b77e-e0a8dd5baab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# --- 2. CONFIGURACI√ìN DEL PROCESO ---\n",
    "REGION = \"us-east-2\"\n",
    "NOMBRE_TABLA = \"nyc_taxi_2025_enero\" \n",
    "FILE_KEY = \"fhvhv_tripdata_2025-01.parquet\"\n",
    "\n",
    "# Ruta temporal local\n",
    "local_path = f\"/tmp/{FILE_KEY.split('/')[-1]}\"\n",
    "try:\n",
    "    # --- PASO 1: Descargar de S3 a Disco Local (/tmp) ---\n",
    "    print(f\"1. Descargando a {local_path} ...\")\n",
    "    if not os.path.exists(local_path):\n",
    "        s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name=REGION)\n",
    "        s3.download_file(BUCKET_NAME, FILE_KEY, local_path)\n",
    "        print(\"‚úÖ Descarga completada.\")\n",
    "    else:\n",
    "        print(\"‚úÖ El archivo ya estaba descargado.\")\n",
    "\n",
    "    # --- PASO 2: Procesamiento por Lotes (Batching) ---\n",
    "    print(f\"2. Escribiendo en la tabla administrada: '{NOMBRE_TABLA}'...\")\n",
    "    \n",
    "    # Abrimos el archivo con PyArrow\n",
    "    parquet_file = pq.ParquetFile(local_path)\n",
    "    \n",
    "    # Lotes peque√±os para que no falle la memoria\n",
    "    total_batches = parquet_file.num_row_groups\n",
    "    print(f\"   Total de grupos a procesar: {total_batches}\")\n",
    "\n",
    "    for i in range(total_batches):\n",
    "        # 1. Leer pedacito con Pandas\n",
    "        batch = parquet_file.read_row_group(i)\n",
    "        pdf = batch.to_pandas()\n",
    "        \n",
    "        # 2. Convertir a Spark\n",
    "        df_chunk = spark.createDataFrame(pdf)\n",
    "        \n",
    "        # 3. GUARDAR COMO TABLA (Aqu√≠ est√° la magia)\n",
    "        if i == 0:\n",
    "            # La primera vez: SOBREESCRIBIMOS la tabla\n",
    "            df_chunk.write.mode(\"overwrite\").saveAsTable(NOMBRE_TABLA)\n",
    "        else:\n",
    "            # Las siguientes veces: AGREGAMOS (Append) a la tabla existente\n",
    "            df_chunk.write.mode(\"append\").saveAsTable(NOMBRE_TABLA)\n",
    "        \n",
    "        # Feedback visual cada 10 lotes para no llenar la consola\n",
    "        if i % 10 == 0:\n",
    "            print(f\"   ‚úÖ Lote {i+1}/{total_batches} guardado en tabla.\")\n",
    "\n",
    "    print(f\"\\nüéâ ¬°VICTORIA! Datos guardados en la tabla '{NOMBRE_TABLA}'\")\n",
    "    \n",
    "    # --- PASO 3: Verificar Resultado ---\n",
    "    # Ahora puedes consultar la tabla usando SQL est√°ndar\n",
    "    print(\"3. Consultando la tabla final...\")\n",
    "    df_final = spark.table(NOMBRE_TABLA)\n",
    "    print(f\"   Total de filas en la tabla: {df_final.count()}\")\n",
    "    display(df_final.limit(5))\n",
    "\n",
    "    # Limpieza\n",
    "    if os.path.exists(local_path):\n",
    "        os.remove(local_path)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8504b0fb-ab00-436c-a965-260b450ec5a1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"request_datetime\":243},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770705649831}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM nyc_taxi_2025_enero LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a55efe7c-0bd7-4418-9daf-c1f33ceffa0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(1) FROM nyc_taxi_2025_enero"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5195668577577339,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingesta_Bronze_Local",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
