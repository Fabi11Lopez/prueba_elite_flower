{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9c8514-1328-42cc-bac2-2b20b4158417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9850c46-848d-4fc4-80ab-aca13129a380",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from botocore.client import Config\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql.functions import current_timestamp # <--- Necesario para el timestamp\n",
    "\n",
    "# --- 2. CONFIGURACIÃ“N DEL PROCESO ---\n",
    "REGION = \"us-east-2\"\n",
    "# Usamos la variable del config para el nombre de la tabla\n",
    "NOMBRE_TABLA = BRONCE_TAXI_TABLE\n",
    "FILE_KEY = \"fhvhv_tripdata_2025-01.parquet\"\n",
    "\n",
    "# Ruta temporal local\n",
    "local_path = f\"/tmp/{FILE_KEY.split('/')[-1]}\"\n",
    "\n",
    "try:\n",
    "    # --- PASO 1: Descargar de S3 a Disco Local (/tmp) ---\n",
    "    print(f\"1. Descargando a {local_path} ...\")\n",
    "    # --- CONFIGURACIÃ“N DE CONEXIÃ“N ROBUSTA ---\n",
    "\n",
    "\n",
    "    # --- AJUSTE DE CONEXIÃ“N SENIOR ---\n",
    "    # Forzamos la regiÃ³n us-east-2 que es la que definiste y la firma v4\n",
    "    try:\n",
    "        print(f\"1. Descargando a {local_path} ...\")\n",
    "        \n",
    "        s3 = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=ACCESS_KEY.strip(),\n",
    "            aws_secret_access_key=SECRET_KEY.strip(),\n",
    "            region_name=\"us-east-2\", # Aseguramos la regiÃ³n del bucket\n",
    "            config=Config(signature_version='s3v4') # Forzamos protocolo de firma v4\n",
    "        )\n",
    "\n",
    "        # Verificamos si el archivo ya existe para evitar descargar basura de intentos fallidos\n",
    "        if os.path.exists(local_path):\n",
    "            os.remove(local_path)\n",
    "\n",
    "        s3.download_file(BUCKET_NAME, FILE_KEY, local_path)\n",
    "        print(\"âœ… Descarga completada con Ã©xito.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error persistente: {str(e)}\")\n",
    "        raise e # Detenemos la ejecuciÃ³n si falla la descarga\n",
    "\n",
    "    # --- PASO 2: Procesamiento por Lotes (Batching) ---\n",
    "    print(f\"2. Escribiendo en la tabla administrada: '{NOMBRE_TABLA}'...\")\n",
    "    \n",
    "    parquet_file = pq.ParquetFile(local_path)\n",
    "    total_batches = parquet_file.num_row_groups\n",
    "    print(f\"   Total de grupos a procesar: {total_batches}\")\n",
    "\n",
    "    for i in range(total_batches):\n",
    "        # 1. Leer lote con Pandas\n",
    "        batch = parquet_file.read_row_group(i)\n",
    "        pdf = batch.to_pandas()\n",
    "        \n",
    "        # 2. Convertir a Spark y agregar columna de auditorÃ­a\n",
    "        df_chunk = spark.createDataFrame(pdf) \\\n",
    "            .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        \n",
    "        # 3. GUARDAR COMO TABLA DELTA\n",
    "        if i == 0:\n",
    "            # Sobreescribimos la primera vez para asegurar el esquema correcto\n",
    "            df_chunk.write.mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(NOMBRE_TABLA)\n",
    "        else:\n",
    "            # Append para los siguientes lotes\n",
    "            df_chunk.write.mode(\"append\") \\\n",
    "                .option(\"mergeSchema\", \"true\") \\\n",
    "                .saveAsTable(NOMBRE_TABLA)\n",
    "        \n",
    "        if i % 10 == 0 or i == total_batches - 1:\n",
    "            print(f\"   âœ… Lote {i+1}/{total_batches} guardado.\")\n",
    "\n",
    "    print(f\"\\nðŸŽ‰ Â¡VICTORIA! Datos en capa Bronze: '{NOMBRE_TABLA}'\")\n",
    "    \n",
    "    # --- PASO 3: Verificar Resultado ---\n",
    "    df_final = spark.table(NOMBRE_TABLA)\n",
    "    print(f\"   Total de filas: {df_final.count()}\")\n",
    "    display(df_final.limit(5))\n",
    "\n",
    "    # Limpieza\n",
    "    if os.path.exists(local_path):\n",
    "        os.remove(local_path)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "3506aee2-e37f-4f18-93a1-5af46d9a6b64",
     "origId": 6991435511570817,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingesta_Bronze_Local_2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
