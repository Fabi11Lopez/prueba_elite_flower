{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6683c8fe-b90f-4af8-9a91-89b78346ab4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definici√≥n de Widgets para par√°metros de entrada\n",
    "# El formato es: dbutils.widgets.text(\"nombre_variable\", \"valor_por_defecto\", \"Etiqueta Visual\")\n",
    "\n",
    "dbutils.widgets.text(\"aws_access_key\", \"\", \"1. AWS Access Key\")\n",
    "dbutils.widgets.text(\"aws_secret_key\", \"\", \"2. AWS Secret Key\")\n",
    "dbutils.widgets.text(\"s3_bucket_name\", \"nombre-de-tu-bucket\", \"3. Nombre del Bucket\")\n",
    "\n",
    "# Widget de lista desplegable para el entorno (esto da un toque Senior)\n",
    "dbutils.widgets.dropdown(\"environment\", \"dev\", [\"dev\", \"qa\", \"prod\"], \"4. Entorno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10fffc74-fdf4-4df7-8310-7848b591b433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Recuperar los valores ingresados por el usuario\n",
    "ACCESS_KEY = dbutils.widgets.get(\"aws_access_key\")\n",
    "SECRET_KEY = dbutils.widgets.get(\"aws_secret_key\")\n",
    "BUCKET_NAME = dbutils.widgets.get(\"s3_bucket_name\")\n",
    "env = dbutils.widgets.get(\"environment\")\n",
    "\n",
    "# Validaci√≥n de seguridad\n",
    "if not ACCESS_KEY or not SECRET_KEY:\n",
    "    dbutils.notebook.exit(\"Error: Debes ingresar las credenciales de AWS en los widgets superiores.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Configuraci√≥n cargada exitosamente para el entorno: {env}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3b78aa-3d94-486b-b77e-e0a8dd5baab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# --- 2. CONFIGURACI√ìN DEL PROCESO ---\n",
    "REGION = \"us-east-2\"\n",
    "NOMBRE_TABLA = \"nyc_taxi_2025_enero\" \n",
    "FILE_KEY = \"fhvhv_tripdata_2025-01.parquet\"\n",
    "\n",
    "# Ruta temporal local\n",
    "local_path = f\"/tmp/{FILE_KEY.split('/')[-1]}\"\n",
    "try:\n",
    "    # --- PASO 1: Descargar de S3 a Disco Local (/tmp) ---\n",
    "    print(f\"1. Descargando a {local_path} ...\")\n",
    "    if not os.path.exists(local_path):\n",
    "        s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name=REGION)\n",
    "        s3.download_file(BUCKET_NAME, FILE_KEY, local_path)\n",
    "        print(\"‚úÖ Descarga completada.\")\n",
    "    else:\n",
    "        print(\"‚úÖ El archivo ya estaba descargado.\")\n",
    "\n",
    "    # --- PASO 2: Procesamiento por Lotes (Batching) ---\n",
    "    print(f\"2. Escribiendo en la tabla administrada: '{NOMBRE_TABLA}'...\")\n",
    "    \n",
    "    # Abrimos el archivo con PyArrow\n",
    "    parquet_file = pq.ParquetFile(local_path)\n",
    "    \n",
    "    # Lotes peque√±os para que no falle la memoria\n",
    "    total_batches = parquet_file.num_row_groups\n",
    "    print(f\"   Total de grupos a procesar: {total_batches}\")\n",
    "\n",
    "    for i in range(total_batches):\n",
    "        # 1. Leer pedacito con Pandas\n",
    "        batch = parquet_file.read_row_group(i)\n",
    "        pdf = batch.to_pandas()\n",
    "        \n",
    "        # 2. Convertir a Spark y AGREGAR columna de auditor√≠a\n",
    "        # Usamos .withColumn para insertar el timestamp actual\n",
    "        df_chunk = spark.createDataFrame(pdf) \\\n",
    "            .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        \n",
    "        # 3. GUARDAR COMO TABLA\n",
    "        if i == 0:\n",
    "            # La primera vez: SOBREESCRIBIMOS la tabla y forzamos el nuevo esquema (con la nueva columna)\n",
    "            df_chunk.write.mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(NOMBRE_TABLA)\n",
    "        else:\n",
    "            # Las siguientes veces: AGREGAMOS y permitimos que el esquema se combine\n",
    "            df_chunk.write.mode(\"append\") \\\n",
    "                .option(\"mergeSchema\", \"true\") \\\n",
    "                .saveAsTable(NOMBRE_TABLA)\n",
    "        \n",
    "        # Feedback visual cada 10 lotes para no llenar la consola\n",
    "        if i % 10 == 0:\n",
    "            print(f\"   ‚úÖ Lote {i+1}/{total_batches} guardado en tabla.\")\n",
    "\n",
    "    print(f\"\\nüéâ ¬°VICTORIA! Datos guardados en la tabla '{NOMBRE_TABLA}'\")\n",
    "    \n",
    "    # --- PASO 3: Verificar Resultado ---\n",
    "    # Ahora puedes consultar la tabla usando SQL est√°ndar\n",
    "    print(\"3. Consultando la tabla final...\")\n",
    "    df_final = spark.table(NOMBRE_TABLA)\n",
    "    print(f\"   Total de filas en la tabla: {df_final.count()}\")\n",
    "    display(df_final.limit(5))\n",
    "\n",
    "    # Limpieza\n",
    "    if os.path.exists(local_path):\n",
    "        os.remove(local_path)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8504b0fb-ab00-436c-a965-260b450ec5a1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"request_datetime\":243},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770705649831}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM nyc_taxi_2025_enero LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a55efe7c-0bd7-4418-9daf-c1f33ceffa0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(1) FROM nyc_taxi_2025_enero;\n",
    "select count(1) FROM bronze_nyc_taxi_2025_enero;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b9db5a3-ff7e-4f23-80ec-6651f4a13699",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"ingestion_timestamp\":272},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770772565136}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURACI√ìN DE RUTAS ---\n",
    "# Usamos las variables que ya definiste en el notebook anterior\n",
    "file_key_zones = \"taxi_zone_lookup.csv\"\n",
    "s3_path_zones = f\"s3a://{BUCKET_NAME}/{file_key_zones}\"\n",
    "nombre_tabla_zonas = \"bronze_taxi_zones\"\n",
    "\n",
    "try:\n",
    "    print(f\"Leyendo cat√°logo de zonas desde: {s3_path_zones}...\")\n",
    "\n",
    "    # --- 2. LECTURA OPTIMIZADA ---\n",
    "    # Para CSV es vital: \n",
    "    # - header=True para tomar los nombres de columnas\n",
    "    # - inferSchema=True para que Spark detecte si son n√∫meros o texto\n",
    "    df_zones = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"sep\", \";\") \\\n",
    "        .load(s3_path_zones)\n",
    "\n",
    "    # --- 3. AUDITOR√çA (Toque Senior) ---\n",
    "    \n",
    "    df_zones_final = df_zones.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "    # --- 4. PERSISTENCIA EN DELTA ---\n",
    "    # Al ser un cat√°logo peque√±o, no necesita particionamiento, \n",
    "    # pero guardarlo en Delta permite hacer JOINs m√°s r√°pidos con los viajes.\n",
    "    df_zones_final.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(nombre_tabla_zonas)\n",
    "\n",
    "    print(f\"‚úÖ Tabla de zonas '{nombre_tabla_zonas}' creada con √©xito.\")\n",
    "    display(spark.table(nombre_tabla_zonas).limit(5))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al cargar las zonas: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8117faed-f899-4e1b-ba73-d3eace4f9610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Limpiar widgets\n",
    "dbutils.widgets.removeAll()\n",
    "print(\"‚ú® Entorno reseteado. Vuelve a ejecutar el config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9699f525-cc3f-48a5-99d7-d590b5cec932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "select count(1) from bronze_taxi_zones"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3730094421533572,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "uso_widgets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
