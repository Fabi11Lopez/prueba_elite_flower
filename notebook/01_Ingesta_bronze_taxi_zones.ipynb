{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea68e0cb-a44c-4d17-9301-034adfaf944c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "879c6326-eb4a-4562-9e94-a137a79bc794",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from botocore.client import Config\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql.functions import current_timestamp, monotonically_increasing_id\n",
    "\n",
    "# Usamos las variables  en el notebook (00_config)\n",
    "file_key_zones = FILE_KEY_ZONES_TABLE\n",
    "s3_path_zones = f\"s3a://{BUCKET_NAME}/{file_key_zones}\"\n",
    "nombre_tabla_zonas = BRONCE_ZONES_TABLE\n",
    "\n",
    "try:\n",
    "    print(f\"Leyendo catálogo de zonas desde: {s3_path_zones}...\")\n",
    "\n",
    "    # --- LECTURA OPTIMIZADA ---\n",
    "    # - header=True para tomar los nombres de columnas\n",
    "    # - inferSchema=True para que Spark detecte si son números o texto\n",
    "    df_zones = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"sep\", \";\") \\\n",
    "        .load(s3_path_zones)\n",
    "\n",
    "    # --- AUDITORÍA---\n",
    "    # Agregamos tanto el timestamp como el ID único que habías importado\n",
    "    df_zones_final = df_zones \\\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"ingestion_id\", monotonically_increasing_id())\n",
    "\n",
    "    # --- PERSISTENCIA EN DELTA ---\n",
    "    # Al ser un catálogo pequeño, no necesita particionamiento, \n",
    "    df_zones_final.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(nombre_tabla_zonas)\n",
    "\n",
    "    print(f\"✅ Tabla de zonas '{nombre_tabla_zonas}' creada con éxito.\")\n",
    "    display(spark.table(nombre_tabla_zonas).limit(5))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al cargar las zonas: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Ingesta_bronze_taxi_zones",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
