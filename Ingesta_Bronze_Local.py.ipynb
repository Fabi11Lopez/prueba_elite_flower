{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3b78aa-3d94-486b-b77e-e0a8dd5baab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# --- 1. CONFIGURACI√ìN ---\n",
    "ACCESS_KEY = \"AKIAVY2PG73H5W3CC2O7\"\n",
    "SECRET_KEY = \"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\"\n",
    "BUCKET_NAME = \"cloudcamp-hire-vehicle-trip-records\"\n",
    "FILE_KEY = \"fhvhv_tripdata_2025-01.parquet\"\n",
    "REGION = \"us-east-2\"\n",
    "\n",
    "# ‚ö†Ô∏è CAMBIO CLAVE: No usamos ruta de archivo, usamos un NOMBRE DE TABLA\n",
    "NOMBRE_TABLA = \"nyc_taxi_2025_enero\" \n",
    "\n",
    "# Ruta temporal local (Python s√≠ puede leer aqu√≠)\n",
    "local_path = f\"/tmp/{FILE_KEY.split('/')[-1]}\"\n",
    "\n",
    "try:\n",
    "    # --- PASO 1: Descargar de S3 a Disco Local (/tmp) ---\n",
    "    print(f\"1. Descargando a {local_path} ...\")\n",
    "    if not os.path.exists(local_path):\n",
    "        s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name=REGION)\n",
    "        s3.download_file(BUCKET_NAME, FILE_KEY, local_path)\n",
    "        print(\"‚úÖ Descarga completada.\")\n",
    "    else:\n",
    "        print(\"‚úÖ El archivo ya estaba descargado.\")\n",
    "\n",
    "    # --- PASO 2: Procesamiento por Lotes (Batching) ---\n",
    "    print(f\"2. Escribiendo en la tabla administrada: '{NOMBRE_TABLA}'...\")\n",
    "    \n",
    "    # Abrimos el archivo con PyArrow\n",
    "    parquet_file = pq.ParquetFile(local_path)\n",
    "    \n",
    "    # Lotes peque√±os para que no falle la memoria\n",
    "    total_batches = parquet_file.num_row_groups\n",
    "    print(f\"   Total de grupos a procesar: {total_batches}\")\n",
    "\n",
    "    for i in range(total_batches):\n",
    "        # 1. Leer pedacito con Pandas\n",
    "        batch = parquet_file.read_row_group(i)\n",
    "        pdf = batch.to_pandas()\n",
    "        \n",
    "        # 2. Convertir a Spark\n",
    "        df_chunk = spark.createDataFrame(pdf)\n",
    "        \n",
    "        # 3. GUARDAR COMO TABLA (Aqu√≠ est√° la magia)\n",
    "        if i == 0:\n",
    "            # La primera vez: SOBREESCRIBIMOS la tabla\n",
    "            df_chunk.write.mode(\"overwrite\").saveAsTable(NOMBRE_TABLA)\n",
    "        else:\n",
    "            # Las siguientes veces: AGREGAMOS (Append) a la tabla existente\n",
    "            df_chunk.write.mode(\"append\").saveAsTable(NOMBRE_TABLA)\n",
    "        \n",
    "        # Feedback visual cada 10 lotes para no llenar la consola\n",
    "        if i % 10 == 0:\n",
    "            print(f\"   ‚úÖ Lote {i+1}/{total_batches} guardado en tabla.\")\n",
    "\n",
    "    print(f\"\\nüéâ ¬°VICTORIA! Datos guardados en la tabla '{NOMBRE_TABLA}'\")\n",
    "    \n",
    "    # --- PASO 3: Verificar Resultado ---\n",
    "    # Ahora puedes consultar la tabla usando SQL est√°ndar\n",
    "    print(\"3. Consultando la tabla final...\")\n",
    "    df_final = spark.table(NOMBRE_TABLA)\n",
    "    print(f\"   Total de filas en la tabla: {df_final.count()}\")\n",
    "    display(df_final.limit(5))\n",
    "\n",
    "    # Limpieza\n",
    "    if os.path.exists(local_path):\n",
    "        os.remove(local_path)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8504b0fb-ab00-436c-a965-260b450ec5a1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"request_datetime\":243},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770705649831}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM nyc_taxi_2025_enero LIMIT 10;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7808942961045681,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingesta_Bronze_Local.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
