# üöï Data Lake Analytics - NYC TLC (HVFHS)

## üìå 1. Objetivo del Proyecto
Este repositorio contiene el dise√±o y c√≥digo fuente para la implementaci√≥n de un Data Lake en AWS utilizando Databricks. El objetivo es procesar y analizar viajes de alto volumen de plataformas como Uber y Lyft (dataset HVFHS de NYC TLC para enero de 2025), garantizando escalabilidad, calidad de datos y buenas pr√°cticas de ingenier√≠a.


## üèóÔ∏è 2. Arquitectura del Data Lake en AWS
La soluci√≥n est√° dise√±ada bajo una **Arquitectura Medall√≥n**, separando el almacenamiento en Amazon S3 y el c√≥mputo distribuido en Databricks (PySpark).


* **Capa Bronze (Ingesta):** Almacenamiento de datos crudos (HVFHS y Cat√°logo de Zonas) tal cual provienen de la fuente. Funciona como un registro hist√≥rico inmutable de tipo *append-only*.
* **Capa Silver (Transformaci√≥n):** Limpieza, normalizaci√≥n de timestamps, casteo estricto de tipos de datos (decimales para m√©tricas financieras) y enriquecimiento espacial mediante *JOIN* con el cat√°logo de zonas.
* **Capa Gold (Presentaci√≥n):** Modelado dimensional y agregaciones diarias para disponibilizar los KPIs de negocio requeridos listos para el consumo de herramientas de BI (como Amazon QuickSight o Athena).

### üíæ Formato de Almacenamiento y Particionado
* **Formato:** Se utiliza **Delta Lake** en las capas Silver y Gold por su soporte nativo de transacciones ACID, evoluci√≥n de esquemas y capacidades de *Time Travel*.
* **Particionado:** La tabla Silver est√° particionada l√≥gicamente por la columna derivada `pickup_date`. Esto optimiza dr√°sticamente los tiempos de lectura y reduce costos computacionales al evitar escaneos completos de la tabla en consultas anal√≠ticas diarias.

---

## ‚öôÔ∏è 3. Desarrollo de ETLs y Reglas de Calidad
El pipeline implementa las siguientes transformaciones cr√≠ticas:
1.  **Detecci√≥n y correcci√≥n de tipos l√≥gicos:** Los campos `base_passenger_fare`, `tolls`, `sales_tax` y dem√°s *fees* se convierten a `decimal(10,2)` para garantizar precisi√≥n financiera.
2.  **Normalizaci√≥n Temporal:** Conversi√≥n de strings a `timestamp` e inferencia de husos horarios para las fechas de *pickup* y *dropoff*.
3.  **Filtros de Integridad:** Se descartan viajes sin zona de origen (`PULocationID` nulo) y viajes il√≥gicos (donde la fecha de fin es menor a la de inicio).
4.  **Generaci√≥n de KPIs (Gold):** C√°lculo preciso de viajes promedio por hora, ingresos totales, tiempo y distancia promedio por d√≠a.

---

## üõ°Ô∏è 4. Estrategia de Incrementalidad, Fallas y Reprocesos
Para asegurar la fiabilidad del Data Lake ante escenarios de producci√≥n, se establecen las siguientes directrices:

* **Idempotencia mediante Partition Overwrite:** El procesamiento Silver implementa un reemplazo din√°mico de particiones (`replaceWhere` en Delta Lake) limitado al periodo procesado (Enero 2025). Esto permite re-ejecutar el pipeline ante fallas sin duplicar informaci√≥n hist√≥rica.
* **Manejo de Errores en Datos:** La ingesta Silver act√∫a como un escudo. Las fechas o formatos inv√°lidos que PySpark no puede castear se eval√∫an, y los duplicados l√≥gicos (misma licencia, fecha y zona) son removidos antes de la escritura mediante `dropDuplicates()`.

### üöÄ Optimizaci√≥n Avanzada: Manejo de Cambios (CDC Conceptual)
Como evoluci√≥n l√≥gica de la arquitectura propuesta, el manejo avanzado de eventos se abordar√° de la siguiente manera:
* **Upserts (Merge):** Transici√≥n de `overwrite` a comandos `MERGE INTO` de Delta Lake para actualizar eficientemente registros de viajes corregidos de forma as√≠ncrona por las plataformas.
* **Llegadas Tard√≠as:** El particionado por `pickup_date` garantiza que los datos rezagados se inserten en su partici√≥n hist√≥rica correcta sin alterar el job del d√≠a actual.
* **Soft Deletes:** En lugar de borrados f√≠sicos por viajes invalidados, se propone una bandera booleana (`is_active = false`) en la capa Silver para mantener trazabilidad.
* **Control de Snapshots:** Aprovechamiento nativo del log de transacciones de Delta Lake para consultar estados pasados (Time Travel) o realizar *Rollbacks*

### üåç Arquitectura y Separaci√≥n de Entornos

Para garantizar el ciclo de vida del desarrollo de software (SDLC) y la integridad de los datos, la arquitectura implementa una **separaci√≥n l√≥gica** mediante prefijos en S3, orquestada por variables de configuraci√≥n din√°micas:

* **üß™ Desarrollo (`dev`)**
    * **Ruta S3:** `s3://datalake/dev/...`
    * **Uso:** Espacio de trabajo transitorio ("Sandbox") para pruebas unitarias, validaci√≥n de c√≥digo y experimentaci√≥n. Los datos en este entorno son vol√°tiles y pueden ser reiniciados en cualquier momento.
    * **Rama Git:** `desarrollo`.

* **üöÄ Producci√≥n (`prod`)**
    * **Ruta S3:** `s3://datalake/prod/...`
    * **Uso:** Fuente √∫nica de verdad. Contiene datos inmutables y fiables que alimentan los dashboards ejecutivos. Solo el pipeline automatizado (Job) tiene permisos de escritura en este prefijo.
    * **Rama Git:** `main`.


### üöß Gesti√≥n de Entornos y Seguridad (Alternativa a Unity Catalog)

Debido a las restricciones t√©cnicas del entorno **Databricks Community Edition** (ausencia de Unity Catalog y persistencia limitada de estados entre contextos de ejecuci√≥n), se opt√≥ por definir las variables de configuraci√≥n de ruta y credenciales de manera expl√≠cita en el c√≥digo (`00_config`). Esta decisi√≥n se tom√≥ para garantizar la **reproducibilidad inmediata** de la Prueba de Concepto (PoC) por parte de los evaluadores, eliminando dependencias de configuraci√≥n externas.

**Propuesta de Arquitectura para Entorno Productivo (AWS):**

En un despliegue empresarial real, esta configuraci√≥n "hardcoded" se reemplazar√≠a por una arquitectura de **Seguridad y Configuraci√≥n Din√°mica** basada en los siguientes componentes:

1.  **Gesti√≥n de Secretos (Databricks Secrets / AWS Secrets Manager):**
    * Las credenciales nunca residir√≠an en el c√≥digo. Se utilizar√≠a la inyecci√≥n de secretos en tiempo de ejecuci√≥n:
        `password = dbutils.secrets.get(scope="production", key="aws-secret-key")`
    * Esto permite rotar las claves en AWS sin detener ni modificar los pipelines.

2.  **Variables de Entorno del Cl√∫ster (Spark Config):**
    * El entorno (`DEV`, `QA`, `PROD`) se definir√≠a a nivel de infraestructura del cl√∫ster (ej: `spark.env = "PROD"`).
    * El c√≥digo leer√≠a esta variable al iniciar y seleccionar√≠a autom√°ticamente la ruta S3 correspondiente (`s3://datalake/{env}/...`), evitando errores humanos de escritura en producci√≥n.

3.  **Seguridad Basada en Roles (IAM Instance Profiles):**
    * Se implementar√≠a el principio de **M√≠nimo Privilegio**. El cl√∫ster de Producci√≥n tendr√≠a un Rol de IAM asociado que solo permite escritura en el bucket `s3://.../prod`, mientras que el cl√∫ster de Desarrollo tendr√≠a acceso restringido √∫nicamente a `s3://.../dev`.