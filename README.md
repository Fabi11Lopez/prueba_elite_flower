# üöï Data Lake Analytics - NYC TLC (HVFHS)

## üìå 1. Objetivo del Proyecto
Este repositorio contiene el dise√±o y c√≥digo fuente para la implementaci√≥n de un Data Lake en AWS utilizando Databricks. El objetivo es procesar y analizar viajes de alto volumen de plataformas como Uber y Lyft (dataset HVFHS de NYC TLC para enero de 2025), garantizando escalabilidad, calidad de datos y buenas pr√°cticas de ingenier√≠a.

---

## üèóÔ∏è 2. Arquitectura del Data Lake en AWS
La soluci√≥n est√° dise√±ada bajo una **Arquitectura Medall√≥n**, separando el almacenamiento en Amazon S3 y el c√≥mputo distribuido en Databricks (PySpark).

![Diagrama de Arquitectura en AWS](docs/arquitectura_aws.png) 

* **Capa Bronze (Ingesta):** Almacenamiento de datos crudos (HVFHS y Cat√°logo de Zonas) tal cual provienen de la fuente. Funciona como un registro hist√≥rico inmutable de tipo *append-only*.
* **Capa Silver (Transformaci√≥n):** Limpieza, normalizaci√≥n de timestamps, casteo estricto de tipos de datos (decimales para m√©tricas financieras) y enriquecimiento espacial mediante *JOIN* con el cat√°logo de zonas.
* **Capa Gold (Presentaci√≥n):** Modelado dimensional y agregaciones diarias para disponibilizar los KPIs de negocio requeridos listos para el consumo de herramientas de BI (como Amazon QuickSight o Athena).

### üíæ Formato de Almacenamiento y Particionado
* **Formato:** Se utiliza **Delta Lake** en las capas Silver y Gold por su soporte nativo de transacciones ACID, evoluci√≥n de esquemas y capacidades de *Time Travel*.
* **Particionado:** La tabla Silver est√° particionada l√≥gicamente por la columna derivada `pickup_date`. Esto optimiza dr√°sticamente los tiempos de lectura y reduce costos computacionales al evitar escaneos completos de la tabla en consultas anal√≠ticas diarias.

---

## ‚öôÔ∏è 3. Desarrollo de ETLs y Reglas de Calidad
El pipeline implementa las siguientes transformaciones cr√≠ticas:
1.  **Detecci√≥n y correcci√≥n de tipos l√≥gicos:** Los campos `base_passenger_fare`, `tolls`, `sales_tax` y dem√°s *fees* se convierten a `decimal(10,2)` para garantizar precisi√≥n financiera.
2.  **Normalizaci√≥n Temporal:** Conversi√≥n de strings a `timestamp` e inferencia de husos horarios para las fechas de *pickup* y *dropoff*.
3.  **Filtros de Integridad:** Se descartan viajes sin zona de origen (`PULocationID` nulo) y viajes il√≥gicos (donde la fecha de fin es menor a la de inicio).
4.  **Generaci√≥n de KPIs (Gold):** C√°lculo preciso de viajes promedio por hora, ingresos totales, tiempo y distancia promedio por d√≠a.

---

## üõ°Ô∏è 4. Estrategia de Incrementalidad, Fallas y Reprocesos
Para asegurar la fiabilidad del Data Lake ante escenarios de producci√≥n, se establecen las siguientes directrices:

* **Idempotencia mediante Partition Overwrite:** El procesamiento Silver implementa un reemplazo din√°mico de particiones (`replaceWhere` en Delta Lake) limitado al periodo procesado (Enero 2025). Esto permite re-ejecutar el pipeline ante fallas sin duplicar informaci√≥n hist√≥rica.
* **Manejo de Errores en Datos:** La ingesta Silver act√∫a como un escudo. Las fechas o formatos inv√°lidos que PySpark no puede castear se eval√∫an, y los duplicados l√≥gicos (misma licencia, fecha y zona) son removidos antes de la escritura mediante `dropDuplicates()`.

### üöÄ Optimizaci√≥n Avanzada: Manejo de Cambios (CDC Conceptual)
Como evoluci√≥n l√≥gica de la arquitectura propuesta, el manejo avanzado de eventos se abordar√° de la siguiente manera:
* **Upserts (Merge):** Transici√≥n de `overwrite` a comandos `MERGE INTO` de Delta Lake para actualizar eficientemente registros de viajes corregidos de forma as√≠ncrona por las plataformas.
* **Llegadas Tard√≠as:** El particionado por `pickup_date` garantiza que los datos rezagados se inserten en su partici√≥n hist√≥rica correcta sin alterar el job del d√≠a actual.
* **Soft Deletes:** En lugar de borrados f√≠sicos por viajes invalidados, se propone una bandera booleana (`is_active = false`) en la capa Silver para mantener trazabilidad.
* **Control de Snapshots:** Aprovechamiento nativo del log de transacciones de Delta Lake para consultar estados pasados (Time Travel) o realizar *Rollbacks*